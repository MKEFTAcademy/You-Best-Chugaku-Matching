#!/usr/bin/env python3
"""
ä¸­å­¦å—é¨“ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Google Custom Search APIã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€Claude APIã§è¦ç´„
"""
import os
import json
import requests
from datetime import datetime, timedelta
from anthropic import Anthropic

# ========================================
# APIè¨­å®š
# ========================================
ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')
GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')
GOOGLE_SEARCH_ENGINE_ID = os.environ.get('GOOGLE_SEARCH_ENGINE_ID')

# ========================================
# æ¤œç´¢è¨­å®š
# ========================================
SEARCH_QUERIES = [
    "ä¸­å­¦å—é¨“ å…¥è©¦",
    "ä¸­å­¦å…¥è©¦ 2026",
    "ç§ç«‹ä¸­å­¦ å‹Ÿé›†",
    "ä¸­å­¦å—é¨“ èª¬æ˜ä¼š"
]

# ========================================
# é–¢æ•°å®šç¾©
# ========================================
def search_news(query, num_results=3):
    """Google Custom Search APIã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
    url = "https://www.googleapis.com/customsearch/v1"
    
    # éå»7æ—¥é–“ã®è¨˜äº‹ã«é™å®š
    date_restrict = "d7"
    
    params = {
        'key': GOOGLE_API_KEY,
        'cx': GOOGLE_SEARCH_ENGINE_ID,
        'q': query,
        'num': num_results,
        'dateRestrict': date_restrict,
        'sort': 'date'
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def summarize_with_claude(title, snippet, link):
    """Claude APIã§è¨˜äº‹ã‚’è¦ç´„"""
    try:
        client = Anthropic(api_key=ANTHROPIC_API_KEY)
        
        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã€ä¸­å­¦å—é¨“ã‚’è€ƒãˆã¦ã„ã‚‹ä¿è­·è€…å‘ã‘ã«100æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã€å‰ç½®ãã‚„èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
æ¦‚è¦: {snippet}
URL: {link}

è¦ç´„:"""
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=200,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        summary = message.content[0].text.strip()
        return summary
        
    except Exception as e:
        print(f"âŒ Claude API ã‚¨ãƒ©ãƒ¼: {e}")
        return snippet[:100] + "..."

def collect_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã—ã¦æ•´å½¢"""
    print("=" * 80)
    print("ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹")
    print("=" * 80)
    
    all_articles = []
    seen_urls = set()
    
    for query in SEARCH_QUERIES:
        print(f"\nğŸ” æ¤œç´¢ä¸­: {query}")
        results = search_news(query)
        
        if not results or 'items' not in results:
            print(f"   âš ï¸  çµæœãªã—")
            continue
        
        for item in results['items']:
            url = item.get('link', '')
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if url in seen_urls:
                continue
            seen_urls.add(url)
            
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            
            print(f"\nğŸ“° è¦ç´„ä½œæˆä¸­: {title[:50]}...")
            summary = summarize_with_claude(title, snippet, url)
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            article = {
                'title': title,
                'summary': summary,
                'source': extract_source(url),
                'date': datetime.now().strftime('%Y-%m-%d'),
                'url': url,
                'category': 'entrance'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ãƒ†ã‚´ãƒª
            }
            
            all_articles.append(article)
            print(f"   âœ… å®Œäº†")
    
    print(f"\nğŸ“Š åé›†å®Œäº†: {len(all_articles)}ä»¶")
    print("=" * 80)
    
    return all_articles

def extract_source(url):
    """URLã‹ã‚‰ã‚½ãƒ¼ã‚¹åã‚’æŠ½å‡º"""
    if 'resemom.jp' in url:
        return 'ãƒªã‚»ãƒãƒ '
    elif 'inter-edu.com' in url:
        return 'ã‚¤ãƒ³ã‚¿ãƒ¼ã‚¨ãƒ‡ãƒ¥'
    elif 'diamond.jp' in url:
        return 'ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
    elif 'president.jp' in url:
        return 'ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
    elif 'benesse.jp' in url:
        return 'ãƒ™ãƒãƒƒã‚»æ•™è‚²æƒ…å ±ã‚µã‚¤ãƒˆ'
    elif 'asahi.com' in url:
        return 'æœæ—¥æ–°èãƒ‡ã‚¸ã‚¿ãƒ«'
    elif 'yomiuri.co.jp' in url:
        return 'èª­å£²æ–°èã‚ªãƒ³ãƒ©ã‚¤ãƒ³'
    elif 'mainichi.jp' in url:
        return 'æ¯æ—¥æ–°è'
    elif 'nikkei.com' in url:
        return 'æ—¥æœ¬çµŒæ¸ˆæ–°è'
    elif 'kyoiku-press.com' in url:
        return 'æ•™è‚²æ–°è'
    elif 'ict-enews.net' in url:
        return 'ICTæ•™è‚²ãƒ‹ãƒ¥ãƒ¼ã‚¹'
    elif 'kyobun.co.jp' in url:
        return 'æ•™è‚²æ–°è'
    elif 'kodomo-it.net' in url:
        return 'å­ä¾›ã¨IT'
    elif 'edtechzine.jp' in url:
        return 'EdTechZine'
    elif 'mext.go.jp' in url:
        return 'æ–‡éƒ¨ç§‘å­¦çœ'
    elif 'syutoken-mosi.co.jp' in url:
        return 'é¦–éƒ½åœæ¨¡è©¦ã‚»ãƒ³ã‚¿ãƒ¼'
    elif 'sapix.co.jp' in url:
        return 'SAPIX'
    elif 'nichinoken.co.jp' in url:
        return 'æ—¥èƒ½ç ”'
    elif 'yotsuyaotsuka.com' in url:
        return 'å››è°·å¤§å¡š'
    elif 'school21.jp' in url:
        return 'ã‚¹ã‚¯ãƒ¼ãƒ«21'
    elif 'tomas.co.jp' in url:
        return 'TOMAS'
    elif 'miraino.org' in url:
        return 'æœªæ¥ã®å­¦æ ¡'
    elif 'study1.jp' in url:
        return 'ã‚¹ã‚¿ãƒ‡ã‚£1'
    else:
        return 'ãã®ä»–'

# ========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================
if __name__ == "__main__":
    # API ã‚­ãƒ¼ã®ç¢ºèª
    if not all([ANTHROPIC_API_KEY, GOOGLE_API_KEY, GOOGLE_SEARCH_ENGINE_ID]):
        print("âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        exit(1)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
    articles = collect_news()
    
    # JSONå‡ºåŠ›
    output_file = 'news_articles.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"ğŸ“ åé›†è¨˜äº‹æ•°: {len(articles)}ä»¶")

